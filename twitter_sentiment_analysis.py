# -*- coding: utf-8 -*-
"""Twitter_sentiment_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ovR2DnwO-0FufvzufLU1ePwdu42yRE28
"""

# Commented out IPython magic to ensure Python compatibility.
# importing required libraries
import pandas as pd
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
import re
import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns
import string
import nltk
import warnings 
warnings.filterwarnings("ignore", category=DeprecationWarning)

# %matplotlib inline

# read the dataset
data = pd.read_csv('twitter_sentiments.csv')
# view the top rows
data.head()

# Describing the dataset
data.describe

# train test split(80-20)
train, test = train_test_split(data, test_size = 0.2, stratify = data['label'], random_state=21)

# get the shape of train and test split.
train.shape, test.shape
## >> ((25569, 3), (6393, 3))

"""Text PreProcessing and Cleaning

Data Inspection

Let's check out a few non racist/sexist tweets.
"""

data['label']

train[train['label'] == 0].head(10)

"""Now check out a few racist/sexist tweets."""

train[train['label'] == 1].head(10)

"""Let's have a glimpse at label-distribution in the train dataset."""

#0 - non racist/sexist tweet
#1 - racist/sexist tweet
train["label"].value_counts()

"""In the train dataset, we have 1794 (~7%)tweets labelled as racist/sexist and 23775(93%) tweets labeled as non racist/sexist. So, it is an imbalanced classification challenge.

Now we will check the distribution of length of the tweets, in terms of words, in both train and test data.
"""

length_train = train['tweet'].str.len()
length_test = test['tweet'].str.len()

plt.hist(length_train, bins=20, label="train_tweets")
plt.hist(length_test, bins=20, label="test_tweets")
plt.legend()
plt.show()

"""Data Cleaning"""

# combinig the test and train dataset
data = train.append(test, ignore_index=True)
data.shape

#user-defined function to remove unwanted text patterns from the tweets.
def remove_pattern(input_txt, pattern):
    r = re.findall(pattern, input_txt)
    for i in r:
        input_txt = re.sub(i, '', input_txt)
        
    return input_txt

"""1. Removing Twitter Handles (@user)"""

data['tidy_tweet'] = np.vectorize(remove_pattern)(data['tweet'], "@[\w]*") 
data.head()

"""2. Removing Punctuations, Numbers, and Special Characters"""

# we do not remove the hashtag
data['tidy_tweet'] = data['tidy_tweet'].str.replace("[^a-zA-Z#]", " ")
data.head(10)

"""3. Removing Short Words"""

# removing the words having lenght shorter than 3
data['tidy_tweet'] = data['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))
data.head(10)

"""4. Text Normalization

Text normalization is the process of transforming a text into a canonical (standard) form. For example, the word “gooood” and “gud” can be transformed to “good”, its canonical form. Another example is mapping of near identical words such as “stopwords”, “stop-words” and “stop words” to just “stopwords”.

Here we will use nltk's PorterStemmer() function to normalize the tweets. But before that we will have to tokenize the tweets. Tokens are individual terms or words, and tokenization is the process of splitting a string of text into tokens.
"""

tokenized_tweet = data['tidy_tweet'].apply(lambda x: x.split()) # tokenizing

tokenized_tweet.head()

"""Now we can normalize the tokenized tweets."""

from nltk.stem.porter import *
stemmer = PorterStemmer()

tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming
#tokenized_tweet.head()

"""Now let’s stitch these tokens back together."""

for i in range(len(tokenized_tweet)):
    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])
    
data['tidy_tweet'] = tokenized_tweet
#data['tidy_tweet'].head()

"""Story Generation and Visualization from Tweets

A) Understanding the common words used in the tweets: WordCloud

Now I want to see how well the given sentiments are distributed across the train dataset. One way to accomplish this task is by understanding the common words by plotting wordclouds.

A wordcloud is a visualization wherein the most frequent words appear in large size and the less frequent words appear in smaller sizes.

Let’s visualize all the words our data using the wordcloud plot.
"""

all_words = ' '.join([text for text in data['tidy_tweet']])
from wordcloud import WordCloud
wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

"""We can see most of the words are positive or neutral. Words like love, great, friend, life are the most frequent ones. It doesn’t give us any idea about the words associated with the racist/sexist tweets. Hence, we will plot separate wordclouds for both the classes (racist/sexist or not) in our train data.

B) Words in non racist/sexist tweets
"""

normal_words =' '.join([text for text in data['tidy_tweet'][data['label'] == 0]])

wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)
plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

"""Most of the frequent words are compatible with the sentiment, i.e, non-racist/sexists tweets. Similarly, we will plot the word cloud for the other sentiment. Expect to see negative, racist, and sexist terms.

C) Racist/Sexist Tweets
"""

negative_words = ' '.join([text for text in data['tidy_tweet'][data['label'] == 1]])
wordcloud = WordCloud(width=800, height=500,
random_state=21, max_font_size=110).generate(negative_words)
plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

"""As we can clearly see, most of the words have negative connotations. So, it seems we have a pretty good text data to work on. Next we will the hashtags/trends in our twitter data.

D) Understanding the impact of Hashtags on tweets sentiment

People use the hashtag symbol (#) before a relevant keyword or phrase in their Tweet to categorize those Tweets and help them show more easily in Twitter search. Clicking or tapping on a hashtagged word in any message shows you other Tweets that include that hashtag.
"""

# function to collect hashtags
def hashtag_extract(x):
    hashtags = []
    # Loop over the words in the tweet
    for i in x:
        ht = re.findall(r"#(\w+)", i)
        hashtags.append(ht)

    return hashtags

# extracting hashtags from non racist/sexist tweets

HT_regular = hashtag_extract(data['tidy_tweet'][data['label'] == 0])

# extracting hashtags from racist/sexist tweets
HT_negative = hashtag_extract(data['tidy_tweet'][data['label'] == 1])

# unnesting list
HT_regular = sum(HT_regular,[])
HT_negative = sum(HT_negative,[])

"""Now that we have prepared our lists of hashtags for both the sentiments, we can plot the top 'n' hashtags. So, first let’s check the hashtags in the non-racist/sexist tweets.

Non-Racist/Sexist Tweets
"""

a = nltk.FreqDist(HT_regular)
d = pd.DataFrame({'Hashtag': list(a.keys()),
                  'Count': list(a.values())})

# selecting top 20 most frequent hashtags     
d = d.nlargest(columns="Count", n = 20) 
plt.figure(figsize=(16,5))
ax = sns.barplot(data=d, x= "Hashtag", y = "Count")
ax.set(ylabel = 'Count')
plt.show()

"""All these hashtags are positive and it makes sense. I am expecting negative terms in the plot of the second list. Let’s check the most frequent hashtags appearing in the racist/sexist tweets.

Racist/Sexist Tweets
"""

b = nltk.FreqDist(HT_negative)
e = pd.DataFrame({'Hashtag': list(b.keys()), 'Count': list(b.values())})

# selecting top 20 most frequent hashtags
e = e.nlargest(columns="Count", n = 20)   
plt.figure(figsize=(16,5))
ax = sns.barplot(data=e, x= "Hashtag", y = "Count")

"""As expected, most of the terms are negative with a few neutral terms as well. So, it’s not a bad idea to keep these hashtags in our data as they contain useful information. Next, we will try to extract features from the tokenized tweets.

Extracting Features from Cleaned Tweets
"""

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
import gensim

"""Bag-of-Words Features"""

# max_df:words coming more than 90 % of entire corpus are removed
# min_df: words coming in less than 2 documents or tweets are removed
bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')
bow = bow_vectorizer.fit_transform(data['tidy_tweet'])
bow.shape

"""TF-IDF Features"""

tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')
tfidf = tfidf_vectorizer.fit_transform(data['tidy_tweet'])
tfidf.shape

"""Word Embeddings
1. Word2Vec Embeddings
"""

tokenized_tweet = data['tidy_tweet'].apply(lambda x: x.split()) # tokenizing

model_w2v = gensim.models.Word2Vec(
            tokenized_tweet,
            size=200, # desired no. of features/independent variables 
            window=5, # context window size
            min_count=2,
            sg = 1, # 1 for skip-gram model
            hs = 0,
            negative = 10, # for negative sampling
            workers= 2, # no.of cores
            seed = 34)

model_w2v.train(tokenized_tweet, total_examples= len(data['tidy_tweet']), epochs=20)

model_w2v.wv.most_similar(positive="trump")

model_w2v['food']

"""Preparing Vectors for Tweets

We will use the below function to create a vector for each tweet by taking the average of the vectors of the words present in the tweet.
"""

def word_vector(tokens, size):
    vec = np.zeros(size).reshape((1, size))
    count = 0.
    for word in tokens:
        try:
            vec += model_w2v[word].reshape((1, size))
            count += 1.
        except KeyError: # handling the case where the token is not in vocabulary
                         
            continue
    if count != 0:
        vec /= count
    return vec

# creating a feature vector of of all tweets in our datset with dimensions 200
# each row represents the vector for that tweet
wordvec_arrays = np.zeros((len(tokenized_tweet), 200))

for i in range(len(tokenized_tweet)):
    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 200)
    
wordvec_df = pd.DataFrame(wordvec_arrays)
wordvec_df.shape
xtrain_w2v , xtest_w2v = train_test_split(wordvec_df, test_size = 0.2, stratify = data['label'], random_state=21)
xtrain_w2v , xtest_w2v

"""Now we have 200 new features, whereas in Bag of Words and TF-IDF we had 1000 features.

2. Doc2Vec Embedding

Let's load the required libraries.

Whether you’re installing software, loading a page or doing a transaction, it always eases your mind whenever you see that small progress bar giving you an estimation of how long the process would take to complete or render. If you have a simple progress bar in your script or code, it looks very pleasing to the eye and gives proper feedback to the user whenever he executes the code. You can use the Python external library tqdm, to create simple & hassle-free progress bars which you can add in your code and make it look lively!
"""

from tqdm import tqdm
tqdm.pandas(desc="progress-bar")
from gensim.models.doc2vec import LabeledSentence

"""To implement doc2vec, we have to labelise or tag each tokenised tweet with unique IDs. We can do so by using Gensim’s LabeledSentence() function."""

def add_label(twt):
    output = []
    for i, s in zip(twt.index, twt):
        output.append(LabeledSentence(s, ["tweet_" + str(i)]))
    return output

labeled_tweets = add_label(tokenized_tweet) # label all the tweets

# testing the labelling process for first 6 tweets
labeled_tweets[:6]

"""Now let's train a doc2vec model."""

model_d2v = gensim.models.Doc2Vec(dm=1, # dm = 1 for ‘distributed memory’ model 
                                  dm_mean=1, # dm = 1 for using mean of the context word vectors
                                  size=200, # no. of desired features
                                  window=5, # width of the context window
                                  negative=7, # if > 0 then negative sampling will be used
                                  min_count=5, # Ignores all words with total frequency lower than 5.
                                  workers=3, # no. of cores
                                  alpha=0.1, # learning rate
                                  seed = 23)

model_d2v.build_vocab([i for i in tqdm(labeled_tweets)])

model_d2v.train(labeled_tweets, total_examples= len(data['tidy_tweet']), epochs=15)

"""Preparing doc2vec Feature Set"""

docvec_arrays = np.zeros((len(tokenized_tweet), 200))

for i in range(len(data)):
    docvec_arrays[i,:] = model_d2v.docvecs[i].reshape((1,200))
    
docvec_df = pd.DataFrame(docvec_arrays)
docvec_df.shape

# each row represents the vector for that tweet
docvec_df

"""Model Building
We are now done with all the pre-modeling stages required to get the data in the proper form and shape. We will be building models on the datasets with different feature sets prepared in the earlier sections — Bag-of-Words, TF-IDF, word2vec vectors, and doc2vec vectors. We will use the following algorithms to build models:

Logistic Regression
Support Vector Machine
RandomForest
XGBoost
1. Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score

"""Bag-of-Words Features"""

bow.shape

train_bow = bow[:25569,:]
test_bow = bow[25569:,:]

# splitting data into training and validation set
xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, train['label'],  
                                                          random_state=42, 
                                                          test_size=0.3)

train_bow

test_bow

xtrain_bow

ytrain

lreg = LogisticRegression()
lreg.fit(xtrain_bow, ytrain) # training the model

prediction = lreg.predict_proba(xvalid_bow) # predicting on the validation set
prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0
prediction_int = prediction_int.astype(np.int)

f1_score(yvalid, prediction_int) # calculating f1 score

"""Now let's make predictions for the test dataset and create a submission file."""

test_pred = lreg.predict_proba(test_bow)
test_pred_int = test_pred[:,1] >= 0.3
test_pred_int = test_pred_int.astype(np.int)
test['label'] = test_pred_int
submission = test[['id','label']]
submission.to_csv('sub_lreg_bow.csv', index=False) # writing data to a CSV file

"""TF-IDF Features"""

train_tfidf = tfidf[:25569,:]
test_tfidf = tfidf[25569:,:]

# splitting data into training and validation set
xtrain_tfidf, xvalid_tfidf, ytrain, yvalid = train_test_split(train_tfidf, train['label'],  
                                                          random_state=42, 
                                                          test_size=0.3)

#xtrain_tfidf = train_tfidf[ytrain.index]
#xvalid_tfidf = train_tfidf[yvalid.index]

lreg.fit(xtrain_tfidf, ytrain)

prediction = lreg.predict_proba(xvalid_tfidf)
prediction_int = prediction[:,1] >= 0.3
prediction_int = prediction_int.astype(np.int)

f1_score(yvalid, prediction_int)

"""Word2Vec Features"""

train_w2v = wordvec_df.iloc[:25569,:]
test_w2v = wordvec_df.iloc[25569:,:]

# splitting data into training and validation set
xtrain_w2v, xvalid_w2v, ytrain, yvalid = train_test_split(train_w2v, train['label'],  
                                                          random_state=42, 
                                                          test_size=0.3)

test_w2v

test_w2v.shape

lreg.fit(xtrain_w2v, ytrain)

prediction = lreg.predict_proba(xvalid_w2v)
prediction_int = prediction[:,1] >= 0.3
prediction_int = prediction_int.astype(np.int)
f1_score(yvalid, prediction_int)

"""Doc2Vec Features"""

train_d2v = docvec_df.iloc[:25569,:]
test_d2v = docvec_df.iloc[25569:,:]

# splitting data into training and validation set
xtrain_d2v, xvalid_d2v, ytrain, yvalid = train_test_split(train_d2v, train['label'],  
                                                          random_state=42, 
                                                          test_size=0.3)

lreg.fit(xtrain_d2v, ytrain)

prediction = lreg.predict_proba(xvalid_d2v)
prediction_int = prediction[:,1] >= 0.3
prediction_int = prediction_int.astype(np.int)
f1_score(yvalid, prediction_int)

"""2. Support Vector Machine"""

from sklearn import svm

"""Bag-of-Words Features"""

svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_bow, ytrain)

prediction = svc.predict_proba(xvalid_bow)
prediction_int = prediction[:,1] >= 0.3
prediction_int = prediction_int.astype(np.int)
f1_score(yvalid, prediction_int)

test_pred = svc.predict_proba(test_bow)
test_pred_int = test_pred[:,1] >= 0.3
test_pred_int = test_pred_int.astype(np.int)
test['label'] = test_pred_int
submission = test[['id','label']]
submission.to_csv('sub_svc_bow.csv', index=False)

"""TF-IDF Features"""

svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_tfidf, ytrain)

prediction = svc.predict_proba(xvalid_tfidf)
prediction_int = prediction[:,1] >= 0.3
prediction_int = prediction_int.astype(np.int)
f1_score(yvalid, prediction_int)

"""Word2Vec Features"""

svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_w2v, ytrain)

prediction = svc.predict_proba(xvalid_w2v)
prediction_int = prediction[:,1] >= 0.3
prediction_int = prediction_int.astype(np.int)
f1_score(yvalid, prediction_int)

"""Doc2Vec Features"""

svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_d2v, ytrain)

prediction = svc.predict_proba(xvalid_d2v)
prediction_int = prediction[:,1] >= 0.3
prediction_int = prediction_int.astype(np.int)
f1_score(yvalid, prediction_int)

"""3. RandomForest"""

from sklearn.ensemble import RandomForestClassifier

"""Bag-of-Words Features"""

rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_bow, ytrain)

prediction = rf.predict(xvalid_bow)
f1_score(yvalid, prediction)

test_pred = rf.predict(test_bow)
test['label'] = test_pred
submission = test[['id','label']]
submission.to_csv('sub_rf_bow.csv', index=False)

"""TF-IDF Features"""

rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_tfidf, ytrain)

prediction = rf.predict(xvalid_tfidf)
f1_score(yvalid, prediction)

"""Word2Vec Features"""

rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_w2v, ytrain)

prediction = rf.predict(xvalid_w2v)
f1_score(yvalid, prediction)

"""Doc2Vec Features"""

rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_d2v, ytrain)

prediction = rf.predict(xvalid_d2v)
f1_score(yvalid, prediction)

"""4. XGBoost"""

from xgboost import XGBClassifier

"""Bag-of-Words Features"""

xgb_model = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_bow, ytrain)
prediction = xgb_model.predict(xvalid_bow)
f1_score(yvalid, prediction)

test_pred = xgb_model.predict(test_bow)
test['label'] = test_pred
submission = test[['id','label']]
submission.to_csv('sub_xgb_bow.csv', index=False)

"""TF-IDF Features"""

xgb = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_tfidf, ytrain)

prediction = xgb.predict(xvalid_tfidf)
f1_score(yvalid, prediction)

"""Word2Vec Features"""

xgb = XGBClassifier(max_depth=6, n_estimators=1000, nthread= 3).fit(xtrain_w2v, ytrain)
prediction = xgb.predict(xvalid_w2v)
f1_score(yvalid, prediction)

"""Doc2Vec Features"""

xgb = XGBClassifier(max_depth=6, n_estimators=1000, nthread= 3).fit(xtrain_d2v, ytrain)
prediction = xgb.predict(xvalid_d2v)
f1_score(yvalid, prediction)

"""BEST RESULT SHOWN BY THE COMBINATION OF XGBOOST AND WORD2VEC

HENCE WE WILL DO HYPERPARAMETER TUNING ON THIS MODEL ONLY
"""

from sklearn.model_selection import GridSearchCV
import xgboost as xgb
gsc = GridSearchCV(
        estimator=XGBClassifier(),
        param_grid = {
    'max_depth':[6,7,8,9,10],
    'min_child_weight':[5,6,7,8],
    #'eta':[.3, .2, .1, .05, .01, .005],
    #'subsample':[0.5,0.6,0.7,0.8,0.9,0.10],
    #'colsample_bytree':[0.5,0.6,0.7,0.8,0.9,0.10]
}
)

best_gsc = gsc.fit(xtrain_w2v, ytrain)

best_gsc

#gsc.grid_scores_
gsc.cv_results_

"""best parameters"""

best_parameters = gsc.best_params_
print(best_parameters)

from sklearn.model_selection import GridSearchCV
import xgboost as xgb
gsc = GridSearchCV(
        estimator=XGBClassifier(),
        param_grid = {
    #'max_depth':[6,7,8,9,10],
    #'min_child_weight':[5,6,7,8],
    #'eta':[.3, .2, .1, .05, .01, .005],
    'subsample':[0.5,0.6,0.7,0.8,0.9,0.10],
    'colsample_bytree':[0.5,0.6,0.7,0.8,0.9,0.10]
}
)

best_gsc = gsc.fit(xtrain_w2v, ytrain)

#gsc.grid_scores_
gsc.cv_results_

"""best parameters"""

best_parameters = gsc.best_params_
print(best_parameters)

from sklearn.model_selection import GridSearchCV
import xgboost as xgb
gsc = GridSearchCV(
        estimator=XGBClassifier(),
        param_grid = {
    #'max_depth':[6,7,8,9,10],
    #'min_child_weight':[5,6,7,8],
    'eta':[.3, .2, .1, .05, .01, .005],
    #'subsample':[0.5,0.6,0.7,0.8,0.9,0.10],
    #'colsample_bytree':[0.5,0.6,0.7,0.8,0.9,0.10]
}
)

best_gsc = gsc.fit(xtrain_w2v, ytrain)

#gsc.grid_scores_
gsc.cv_results_

best_parameters = gsc.best_params_
print(best_parameters)

from sklearn.model_selection import GridSearchCV
import xgboost as xgb
gsc = GridSearchCV(
        estimator=XGBClassifier(),
        param_grid = {
    #'max_depth':[6,7,8,9,10],
    #'min_child_weight':[5,6,7,8],
    #'eta':[.3, .2, .1, .05, .01, .005],
    'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05]

    #'subsample':[0.5,0.6,0.7,0.8,0.9,0.10],
    #'colsample_bytree':[0.5,0.6,0.7,0.8,0.9,0.10]
}
)

best_gsc = gsc.fit(xtrain_w2v, ytrain)

#gsc.grid_scores_
gsc.cv_results_

best_parameters = gsc.best_params_
print(best_parameters)

xgb_model = XGBClassifier(max_depth = 9, min_child_weight=  7, n_estimators = 1000 , colsample_bytree =  0.8, subsample =  0.5, eta = 0.3 , reg_alpha = 0.001).fit(xtrain_w2v, ytrain)

test_pred = xgb_model.predict(test_w2v)
f1_score(test['label'], test_pred)

test_pred = xgb_model.predict(test_w2v)
test['label'] = (test_pred >= 0.3).astype(np.int)
submission = test[['id','label']]
submission.to_csv('sub_xgb_w2v.csv', index=False)

